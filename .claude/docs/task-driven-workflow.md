# Task-Driven Migration Workflow v4.3

## ðŸ†• Overview

This document defines the **Task-Driven Migration Workflow** where tasks are **pre-generated by an external framework** and provided as JSON files.

**Key Changes from v4.2:**
- âŒ **NO SDD Analysis** - Tasks already generated
- âŒ **NO Contract Generation** - Contracts defined in task descriptions
- âœ… **Task Import & Validation** - Load and validate 40 pre-generated tasks
- âœ… **Agent Assignment** - Map tasks to specialized agents
- âœ… **Sequential Execution** - Execute tasks respecting dependencies
- âœ… **100% Traceability** - Track every task from start to completion

---

## ðŸ“‚ Input Files

**Location**: `docs/input/`

1. **`ai_agent_tasks.json`** - 30 base tasks (TASK-001 to TASK-030)
2. **`ai_agent_tasks_extended.json`** - 10 additional tasks (TASK-031 to TASK-040)

**Total**: 40 tasks covering:
- Frontend setup (Next.js)
- Backend setup (FastAPI)
- Database schema (PostgreSQL)
- Authentication & Security
- Business logic
- Testing (unit, integration, E2E, performance, security)
- Documentation

---

## ðŸ“‹ Task Structure

Each task contains:

```json
{
  "id": "TASK-001",
  "title": "Initialize Next.js Frontend Project Structure",
  "description": "Detailed implementation instructions...",
  "suggestedPhase": "PHASE-01",
  "priority": "high",
  "effortEstimate": "30 minutes",
  "dependencies": [],
  "acceptanceCriteria": [
    "Next.js project created with TypeScript",
    "Directory structure matches specification",
    ...
  ],
  "relatedRequirements": ["NFR-5", "NFR-17"],
  "skillsRequired": ["Next.js", "TypeScript"],
  "deliverables": [
    "frontend/package.json",
    "frontend/tsconfig.json",
    ...
  ]
}
```

---

## ðŸ”„ NEW WORKFLOW PHASES

### PHASE 0: Task Import & Validation (NEW)

**Orchestrator Actions:**

1. **Read Input Files**
   ```bash
   Read: docs/input/ai_agent_tasks.json
   Read: docs/input/ai_agent_tasks_extended.json
   ```

2. **Merge Tasks**
   ```python
   all_tasks = base['tasks'] + extended['additional_tasks']
   # Total: 40 tasks
   ```

3. **Validate Task Structure**
   - All 40 tasks present
   - No duplicate IDs
   - All required fields exist (id, title, description, dependencies, deliverables)
   - Dependency references valid (no orphan dependencies)

4. **Build Dependency Graph**
   ```python
   dependency_graph = {}
   for task in all_tasks:
       dependency_graph[task['id']] = task['dependencies']

   # Example:
   # TASK-001: []
   # TASK-002: []
   # TASK-003: [TASK-002]
   # TASK-004: [TASK-002, TASK-003]
   ```

5. **Calculate Execution Order (Topological Sort)**
   ```python
   execution_levels = {
       0: [TASK-001, TASK-002],  # No dependencies
       1: [TASK-003, TASK-005, TASK-006],  # Depend on level 0
       2: [TASK-004, TASK-007, TASK-008],  # Depend on level 1
       ...
   }
   ```

6. **Map Tasks to Framework Phases**
   ```python
   phase_mapping = {
       'PHASE-01': 'PHASE-1',  # Contracts & Setup
       'PHASE-02': 'PHASE-2',  # Domain & Application
       'PHASE-03': 'PHASE-3',  # Infrastructure
       'PHASE-04': 'PHASE-4',  # Testing
       'PHASE-05': 'PHASE-5',  # Delivery
   }
   ```

7. **Map Tasks to Agents**
   ```python
   agent_assignment = {}
   for task in all_tasks:
       agent = determine_agent(task)
       agent_assignment[task['id']] = agent

   # Auto-mapping logic:
   # - deliverables contain 'models/' â†’ infrastructure-agent
   # - deliverables contain 'schemas/' â†’ use-case-agent
   # - description contains 'business logic' â†’ domain-agent
   # - deliverables contain 'tests/' â†’ qa-test-generator or e2e-qa-agent
   ```

8. **Generate `tasks.json`**
   ```json
   {
     "project_name": "Legacy Banking System Modernization",
     "total_tasks": 40,
     "execution_order": {
       "level_0": ["TASK-001", "TASK-002"],
       "level_1": ["TASK-003", "TASK-005", ...],
       ...
     },
     "tasks": [
       {
         "id": "TASK-001",
         "original_phase": "PHASE-01",
         "framework_phase": "PHASE-1",
         "layer": "infrastructure_frontend",
         "assigned_agent": "infrastructure-agent",
         "status": "pending",
         "owner": null,
         "started_at": null,
         "completed_at": null,
         "dependencies": [],
         "dependencies_met": true,
         "title": "Initialize Next.js Frontend Project Structure",
         "description": "...",
         "acceptanceCriteria": [...],
         "deliverables": [...],
         "test_strategy": null  // Added by qa-test-generator
       },
       ...
     ]
   }
   ```

9. **Generate Execution Plan**
   ```markdown
   # Execution Plan

   ## Statistics
   - Total tasks: 40
   - Dependency levels: 8
   - Estimated time: 32 hours

   ## Execution Order

   ### Level 0 (No dependencies - 2 tasks)
   - TASK-001: Initialize Next.js Frontend (infrastructure-agent)
   - TASK-002: Initialize Python Backend (infrastructure-agent)

   ### Level 1 (Depends on Level 0 - 4 tasks)
   - TASK-003: Setup PostgreSQL Schema (infrastructure-agent)
   - TASK-005: Create Pydantic Schemas (use-case-agent)
   - TASK-006: Implement JWT Authentication (infrastructure-agent)

   ...

   ## Agent Workload
   - infrastructure-agent: 22 tasks (55%)
   - use-case-agent: 9 tasks (22.5%)
   - domain-agent: 3 tasks (7.5%)
   - qa-test-generator: 4 tasks (10%)
   - e2e-qa-agent: 2 tasks (5%)
   ```

10. **User Confirmation**
    ```
    ðŸ“Š Task Import Complete

    - Tasks loaded: 40
    - Execution levels: 8
    - Estimated time: 32 hours
    - Agent assignments: 5 agents

    Execution plan saved to: docs/state/execution-plan.md

    âœ… Ready to start implementation

    Continue? (yes/no)
    ```

**Output Files:**
- `docs/state/tasks.json` - All 40 tasks with framework metadata
- `docs/state/dependency-graph.json` - Dependency graph visualization
- `docs/state/execution-plan.md` - Human-readable execution plan
- `docs/state/agent-assignments.json` - Task assignments per agent

---

### PHASE 0.5: Tech Stack Validation (OPTIONAL - Can Skip)

**Reason to Skip**: Tasks already specify exact tech stack (Next.js, FastAPI, PostgreSQL, etc.)

**IF you want to validate anyway:**
- Verify Next.js version compatibility
- Verify FastAPI version compatibility
- Check for known library incompatibilities (e.g., Radix UI + Playwright issue from v4.3 lessons)

---

### PHASE 0.8: Test Strategy Enrichment (TDD)

**Agent**: qa-test-generator

**Mission**: Read `tasks.json` and add test strategies for each task.

**For each task:**
1. Read `acceptanceCriteria`
2. Generate test specifications:
   - **Unit tests**: What to test, how to test, expected results
   - **Integration tests**: Component interactions, API contracts
   - **E2E tests**: User workflows, critical paths

3. Update `tasks.json`:
   ```json
   {
     "id": "TASK-004",
     "title": "Create SQLAlchemy Models",
     "test_strategy": {
       "unit_tests": [
         {
           "test_name": "test_customer_model_validation",
           "description": "Test Customer model validates title from allowed list",
           "assertions": [
             "Valid titles accepted (Mr, Mrs, Miss, etc.)",
             "Invalid title raises ValueError",
             "Date of birth > 1600 validated"
           ]
         }
       ],
       "integration_tests": [
         {
           "test_name": "test_customer_account_cascade_delete",
           "description": "Test deleting customer cascades to accounts",
           "setup": "Create customer with 3 accounts",
           "action": "Delete customer",
           "assertion": "All 3 accounts deleted"
         }
       ]
     }
   }
   ```

**Output**: `docs/state/tasks.json` updated with test strategies for all 40 tasks.

---

### PHASE 1-3: Sequential Task Execution

**Orchestrator Logic:**

```python
def execute_migration():
    tasks = load_tasks("docs/state/tasks.json")
    execution_order = calculate_execution_order(tasks)

    for level in execution_order:
        for task_id in level:
            task = tasks[task_id]

            # Check dependencies met
            if not all_dependencies_completed(task):
                log_error(f"Dependencies not met for {task_id}")
                continue

            # Determine agent
            agent = task['assigned_agent']

            # Invoke agent
            invoke_agent(agent, task)

            # Wait for completion
            wait_for_task_completion(task_id)

            # Validate completion
            if not validate_task_completion(task):
                log_error(f"Task {task_id} validation failed")
                break

    # All tasks completed
    log_success("All 40 tasks completed successfully")
```

**Agent Invocation Pattern:**

```python
Task(
    description=f"Implement {task['id']}: {task['title']}",
    prompt=f"""
    Read .claude/agents/{agent_name}.md for your instructions.

    **YOUR MISSION**: Implement the following task from the pre-generated task list.

    **Task Details**:
    - ID: {task['id']}
    - Title: {task['title']}
    - Description: {task['description']}
    - Dependencies: {task['dependencies']} (all completed)

    **Deliverables Required**:
    {json.dumps(task['deliverables'], indent=2)}

    **Acceptance Criteria**:
    {json.dumps(task['acceptanceCriteria'], indent=2)}

    **Test Strategy**:
    {json.dumps(task['test_strategy'], indent=2)}

    **CRITICAL RULES**:
    1. Follow the task description EXACTLY
    2. Create ALL deliverables listed
    3. Meet ALL acceptance criteria
    4. Implement tests according to test strategy
    5. Update tasks.json when you claim the task (set owner and status)
    6. Update tasks.json when you complete the task
    7. Write progress notes in docs/state/tracking/{agent_name}-progress.json

    **When complete**: Report back with:
    - Files created/modified
    - Tests implemented
    - Acceptance criteria met
    - Any blockers or issues
    """,
    subagent_type="Explore",  # or specific agent type
    model="sonnet"
)
```

---

### PHASE 4: Testing (Smoke Tests + E2E)

**After all implementation tasks complete:**

1. **PHASE 4.5: Smoke Tests** (if applicable)
   - Run 6 critical API tests
   - Must pass 100% before E2E

2. **PHASE 4: E2E Tests**
   - Tasks with `deliverables` containing `tests/e2e/` or `tests/performance/`
   - Execute via e2e-qa-agent
   - Max 3 iterations per v4.3 rules

---

### PHASE 5: Delivery & Validation

1. **Verify All Tasks Completed**
   ```python
   tasks = load_tasks("docs/state/tasks.json")
   completed = [t for t in tasks if t['status'] == 'completed']
   assert len(completed) == 40, "Not all tasks completed!"
   ```

2. **Generate Final Report**
   ```markdown
   # Migration Complete

   ## Statistics
   - Total tasks: 40
   - Completed: 40
   - Success rate: 100%
   - Total time: 28 hours

   ## Deliverables
   - Frontend files: 45
   - Backend files: 78
   - Test files: 32
   - Documentation files: 12

   ## Test Results
   - Unit tests: 245/245 passed
   - Integration tests: 67/67 passed
   - E2E tests: 18/18 passed
   - Performance tests: PASSED (95th percentile < 200ms)
   - Security tests: PASSED (0 vulnerabilities)
   ```

3. **Archive Task List**
   ```bash
   cp docs/state/tasks.json docs/state/tasks-completed-$(date +%Y%m%d).json
   ```

---

## ðŸŽ¯ Task Lifecycle

```
pending â†’ claimed â†’ in_progress â†’ completed
   â†“         â†“           â†“             â†“
[waiting] [owner]  [implementing]  [validated]
```

**State Transitions:**

1. **pending â†’ claimed**
   - Agent reads task from `tasks.json`
   - Sets `owner: "infrastructure-agent"`
   - Sets `status: "claimed"`
   - Sets `started_at: timestamp`

2. **claimed â†’ in_progress**
   - Agent starts implementation
   - Sets `status: "in_progress"`
   - Creates progress file in `docs/state/tracking/{agent}-progress.json`

3. **in_progress â†’ completed**
   - All deliverables created
   - All acceptance criteria met
   - Tests pass
   - Sets `status: "completed"`
   - Sets `completed_at: timestamp`

---

## ðŸ“Š Traceability Guarantees

### 1. No Task Omitted

**Validation at PHASE 0:**
```python
# Load input files
base_tasks = json.load('ai_agent_tasks.json')['tasks']  # 30 tasks
extended_tasks = json.load('ai_agent_tasks_extended.json')['additional_tasks']  # 10 tasks

# Verify count
assert len(base_tasks) == 30, "Expected 30 base tasks"
assert len(extended_tasks) == 10, "Expected 10 extended tasks"
assert len(all_tasks) == 40, "Expected 40 total tasks"

# Verify IDs sequential
expected_ids = [f"TASK-{str(i).zfill(3)}" for i in range(1, 41)]
actual_ids = [t['id'] for t in all_tasks]
missing = set(expected_ids) - set(actual_ids)
assert len(missing) == 0, f"Missing tasks: {missing}"
```

**Validation at PHASE 5:**
```python
# Verify all tasks completed
completed_tasks = [t for t in tasks if t['status'] == 'completed']
assert len(completed_tasks) == 40, f"Only {len(completed_tasks)}/40 tasks completed"

# Generate completion report
completion_report = {
    'total': 40,
    'completed': len(completed_tasks),
    'failed': len([t for t in tasks if t['status'] == 'failed']),
    'skipped': len([t for t in tasks if t['status'] == 'skipped']),
    'missing': [t['id'] for t in tasks if t['status'] not in ['completed', 'failed', 'skipped']]
}

assert completion_report['completed'] == 40, "Not all tasks completed!"
```

### 2. Dependency Order Respected

```python
def validate_dependency_order():
    for task in tasks:
        if task['status'] == 'in_progress' or task['status'] == 'completed':
            for dep_id in task['dependencies']:
                dep_task = tasks[dep_id]
                assert dep_task['status'] == 'completed', \
                    f"{task['id']} started before dependency {dep_id} completed"
```

### 3. All Deliverables Created

```python
def validate_deliverables(task):
    for file_path in task['deliverables']:
        assert os.path.exists(file_path), \
            f"Deliverable missing: {file_path} for {task['id']}"

    # Update task
    task['deliverables_created'] = True
```

### 4. All Acceptance Criteria Met

```python
def validate_acceptance_criteria(task):
    # Run automated checks
    results = []
    for criterion in task['acceptanceCriteria']:
        # Parse criterion and run validation
        result = validate_criterion(criterion)
        results.append({'criterion': criterion, 'met': result})

    # Update task
    task['acceptance_results'] = results
    task['all_criteria_met'] = all(r['met'] for r in results)
```

---

## ðŸ”§ Agent Modifications Required

### infrastructure-agent

**Changes:**
- Read task from `tasks.json` instead of generating from FDD
- Claim task by setting `owner` and `status`
- Implement exactly per task `description`
- Create all files in `deliverables`
- Update progress in `docs/state/tracking/infrastructure-agent-progress.json`
- Mark task as `completed` when done

### use-case-agent

**Changes:**
- Same as infrastructure-agent
- Focus on tasks with `deliverables` containing `schemas/` or `services/`

### domain-agent

**Changes:**
- Same as infrastructure-agent
- Focus on tasks with business logic keywords in `description`

### qa-test-generator

**Changes:**
- NEW MODE: "Test Strategy Enrichment"
- Reads all tasks from `tasks.json`
- For each task, generates test strategy
- Updates `tasks.json` with test specifications
- Does NOT implement tests (that's done by the agent implementing the task)

### e2e-qa-agent

**Changes:**
- Reads tasks with E2E test deliverables
- Implements E2E tests per task specification
- Reports results back to orchestrator

---

## ðŸ“ File Structure Changes

```
docs/
â”œâ”€â”€ input/                              # NEW - Input from other framework
â”‚   â”œâ”€â”€ ai_agent_tasks.json             # 30 base tasks
â”‚   â””â”€â”€ ai_agent_tasks_extended.json    # 10 extended tasks
â”œâ”€â”€ state/
â”‚   â”œâ”€â”€ tasks.json                      # 40 tasks with framework metadata
â”‚   â”œâ”€â”€ dependency-graph.json           # Dependency visualization
â”‚   â”œâ”€â”€ execution-plan.md               # Human-readable plan
â”‚   â”œâ”€â”€ agent-assignments.json          # Task assignments
â”‚   â”œâ”€â”€ global-state.json               # Overall migration state
â”‚   â””â”€â”€ tracking/                       # Agent progress tracking
â”‚       â”œâ”€â”€ infrastructure-agent-progress.json
â”‚       â”œâ”€â”€ use-case-agent-progress.json
â”‚       â”œâ”€â”€ domain-agent-progress.json
â”‚       â”œâ”€â”€ qa-test-generator-progress.json
â”‚       â””â”€â”€ e2e-qa-agent-progress.json
â””â”€â”€ ...
```

---

## âœ… Success Criteria

Migration successful when:

1. âœ… All 40 tasks status = "completed"
2. âœ… All 40 * N deliverables created (files exist)
3. âœ… All acceptance criteria met (100%)
4. âœ… All tests pass (unit + integration + E2E)
5. âœ… No task omitted (verified by ID checklist)
6. âœ… Dependency order respected (validated at each step)
7. âœ… Code runs locally without errors

---

## ðŸš¨ Error Handling

### Task Dependency Not Met

```python
if not all_dependencies_completed(task):
    log_warning(f"Task {task_id} blocked: dependencies not met")
    task['status'] = 'blocked'
    task['blocked_reason'] = f"Waiting for: {incomplete_dependencies}"
    # Skip and continue to next task
```

### Task Implementation Failed

```python
if task_implementation_failed(task):
    log_error(f"Task {task_id} failed: {error}")
    task['status'] = 'failed'
    task['error_message'] = str(error)

    # Ask user
    decision = ask_user_strategic_decision(task)

    if decision == 'retry':
        retry_task(task)
    elif decision == 'skip':
        task['status'] = 'skipped'
    elif decision == 'abort':
        abort_migration()
```

### Missing Deliverable

```python
if not all_deliverables_exist(task):
    missing = get_missing_deliverables(task)
    log_error(f"Task {task_id} incomplete: missing {missing}")

    # Auto-fix: Re-invoke agent to create missing files
    re_invoke_agent(task, focus_on=missing)
```

---

## ðŸŽ¯ Key Advantages of Task-Driven Approach

1. **No SDD Analysis Overhead** - Tasks pre-generated, saves 2-4 hours
2. **No Contract Generation** - Contracts in task descriptions, saves 1-2 hours
3. **100% Traceability** - Every task tracked from start to finish
4. **Predictable Timeline** - 40 tasks with effort estimates = accurate ETA
5. **Scalable** - Easy to add more tasks (TASK-041, TASK-042, ...)
6. **Agent Specialization** - Each agent focuses on their tasks only
7. **Parallel Execution Possible** - Tasks at same dependency level can run in parallel
8. **Clear Validation** - Acceptance criteria = automated validation

---

## ðŸ“ Example: Full Task Execution

**Task**: TASK-004 (Create SQLAlchemy Models)

**1. Orchestrator reads task:**
```json
{
  "id": "TASK-004",
  "title": "Create SQLAlchemy Database Models",
  "dependencies": ["TASK-002", "TASK-003"],
  "assigned_agent": "infrastructure-agent",
  "status": "pending"
}
```

**2. Check dependencies:**
```python
TASK-002: status = "completed" âœ“
TASK-003: status = "completed" âœ“
â†’ Dependencies met, can execute
```

**3. Invoke infrastructure-agent:**
```python
Task(
    description="Implement TASK-004",
    prompt="""
    **YOUR MISSION**: Create SQLAlchemy models per TASK-004 specification.

    Read the task description from tasks.json.
    Create the following files:
    - backend/app/models/customer.py
    - backend/app/models/account.py
    - backend/app/models/transaction.py
    - backend/app/models/__init__.py
    - backend/app/db/session.py

    Acceptance criteria:
    - Customer model with all fields and validations
    - Title validation enforces 10 valid values
    - Age validation ensures < 150 years
    - Account model with FK to Customer
    - ... (11 more criteria)

    Update tasks.json:
    1. Set owner="infrastructure-agent"
    2. Set status="in_progress"
    3. When done, set status="completed"
    """,
    subagent_type="Explore"
)
```

**4. Agent executes:**
- Claims task (sets owner, status)
- Creates 5 files
- Implements validations
- Writes tests
- Updates tasks.json
- Reports completion

**5. Orchestrator validates:**
```python
# Check all deliverables exist
assert exists("backend/app/models/customer.py")
assert exists("backend/app/models/account.py")
assert exists("backend/app/models/transaction.py")
assert exists("backend/app/models/__init__.py")
assert exists("backend/app/db/session.py")

# Check acceptance criteria
assert task['status'] == 'completed'
assert task['all_criteria_met'] == True

# Move to next task
next_task = "TASK-005"
```

---

## ðŸ”„ Summary: Old vs New Workflow

| Phase | Old Workflow (v4.2) | New Workflow (v4.3 Task-Driven) |
|-------|---------------------|--------------------------------|
| **0** | SDD Analysis (2-4 hours) | Task Import & Validation (15 min) |
| **0.5** | Tech Stack Validation | Optional (can skip) |
| **0.7** | Task Generation | Not needed (tasks pre-generated) |
| **0.8** | TDD Test Specs | Test Strategy Enrichment |
| **1** | Contract Generation | Not needed (in task descriptions) |
| **2-3** | Implementation | Sequential Task Execution (40 tasks) |
| **4.5** | Smoke Tests | Same |
| **4** | E2E Tests | Same |
| **5** | Delivery | Same + Task Completion Report |

**Time Saved**: 3-6 hours per migration (SDD analysis + contract generation)

---

**Ready to implement this workflow!** ðŸš€
